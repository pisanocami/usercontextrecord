1) Brand Identity â€” ValidaciÃ³n â€œNo AmbigÃ¼edadâ€
âœ… Inputs mÃ­nimos requeridos

 Dominio (client_domain)

 PaÃ­s / mercado (US, EU, etc.)

 Modelo de negocio (DTC / SaaS / Marketplace / Hybrid)

 Primary category (seleccionada o confirmada)

âœ… Reglas de calidad

 Category + Alternatives deben ser editables y guardarse como approved_categories

 Si la IA propone alternativas, deben venir con:

 razÃ³n (â€œpor quÃ© cree estoâ€)

 evidencia (ej: snippet SERP / tÃ©rmino Trends / top ranking keywords)

 El sistema debe generar: context_hash (huella determinÃ­stica)

ğŸ“Œ Meta: que el contexto sea â€œfrozen and reproducibleâ€.

2) Competitive Set â€” El punto mÃ¡s crÃ­tico

Susan tiene razÃ³n: el riesgo #1 es que la IA elija competidores incorrectos.

âœ… Campos a agregar (para hacerlo perfecto)

 Competitor Tiering

Tier 1 = direct (misma categorÃ­a + mismo precio/segmento)

Tier 2 = adjacent

Tier 3 = aspirational/giant

 Company Size Guardrails (lo que Susan pide)

 revenue range (si estÃ¡ disponible)

 employee count

 funding stage / public vs private

 geo overlap

âœ… Reglas de calidad (no negociables)

 La IA debe devolver Competitor Candidates con score:

similarity score (category overlap)

SERP overlap score

ad overlap (si estÃ¡ disponible)

size proximity score

 El usuario puede â€œapproveâ€ competidores

 El sistema guarda:

approved_competitors

rejected_competitors

unknown_competitors_pending_review

âœ… Plan B (si IA falla)

 fallback: Ahrefs / SEMrush / Similarweb competitor suggestions

 fallback: SERP-based co-occurrence (quiÃ©n aparece al buscar keywords core)

 fallback: manual list upload (CSV)

ğŸ“Œ Meta: IA propone â†’ humano valida (solo si se requiere) â†’ contexto queda locked.

3) Negative Scope â€” El escudo anti-poluciÃ³n

Esto estÃ¡ MUY bien planteado en tu arquitectura.

âœ… Debe incluir 4 bloques mÃ­nimo

 excluded_keywords (string list)

 excluded_categories (string list)

 excluded_competitors (domain list)

 exclusion_reasoning (texto simple)

âœ… Mejoras â€œperfectasâ€

 Soportar dos tipos:

Exact match

Semantic match (v2)

 Campo adicional:

semantic_sensitivity: low/medium/high

 Campo adicional:

override_expires_at obligatorio para overrides

âœ… Enforcement (CMO-safe)

 Pre-filter gate antes de APIs (para reducir costos)

 Post-validation despuÃ©s de LLM

 Audit log por cada item filtrado

 El output final incluye: filters_applied

ğŸ“Œ Meta: nunca pagar por datos que vas a descartar.

4) Strategic Intent â€” Para que el scoring sea coherente

Tu UI ya lo tiene (Growth Priority, Risk Tolerance).

âœ… Campos mÃ­nimos

 growth_priority: aggressive/balanced/conservative

 risk_tolerance: high/medium/low

âœ… Perfecto si agregÃ¡s:

 goal_type: ROI / Volume / Authority (como tu diagrama)

 time_horizon: 30/60/90+ dÃ­as

 constraint flags:

â€œNo new contentâ€

â€œOnly optimize existing pagesâ€

â€œBrand compliance strictâ€

ğŸ“Œ Meta: que el sistema recomiende cosas que el operador pueda ejecutar.

5) Context Validation â€” Fail Closed pero sin bloquear producciÃ³n

AcÃ¡ estÃ¡ el matiz: human review no frena generaciÃ³n, pero:

si el contexto estÃ¡ incompleto â†’ el sistema corre con bandera needs_review

si hay violaciÃ³n â†’ fail closed

âœ… Reglas

 Si falta negative_scope â†’ BLOCK

 Si falta categorÃ­a primaria â†’ BLOCK

 Si hay menos de 3 competidores tier1 â†’ WARNING, pero corre

 Si no fue revisado por humano â†’ output con confidence_band downgraded

ğŸ“Œ Meta: producir reportes sin estrellarse, pero nunca entregar algo inseguro.

6) â€œContext Quality Scoreâ€ (para tranquilidad de Susan)

Esto es CLAVE: un score tipo â€œAI Confidence 95/100â€ no sirve si no sabemos por quÃ©.

âœ… Score compuesto recomendado

 Completeness score (campos llenos)

 Competitor confidence score

 Negative scope strength score

 Evidence coverage score

 Human verified flag (sÃ­/no)

Output visible (ejemplo):

Context Quality: High / Medium / Low

Competitor Set Confidence: Medium (3/6 verified)

Human Review: Pending

This report is generated with safe constraints âœ…

ğŸ“Œ Meta: â€œCMO-safe by constructionâ€.

7) Evidence Pack (para que sea defendible)

Todo output deberÃ­a poder explicar:

âœ… de dÃ³nde saliÃ³ cada cosa.

âœ… Por cada competidor:

 why_selected

 top_overlap_keywords (5)

 SERP overlap examples (3)

âœ… Por cada category alternative:

 supporting queries

 volume/velocity evidence

ğŸ“Œ Meta: que el CMO lo pueda defender en la reuniÃ³n.

8) Versioning + Determinismo

Esto completa tu contrato.

 context_version

 context_hash

 outputs guardan ambos

 si cambia contexto, se crea nueva versiÃ³n

ğŸ“Œ Meta: reproducibilidad total.

9) AI Behavior Contract (lo que Susan quiere que definamos)

Esto se define en 4 decisiones:

âœ… A) Exclusion matching

 v1 = exact match + partial substring

 v2 = semantic match configurable

âœ… B) Competitors wrong

 IA propone + score

 si confidence < threshold â†’ pide validaciÃ³n humana

âœ… C) Context incomplete

 No bloquea reporte, pero:

band de confianza baja

warning visible

requiere review para â€œlockedâ€

âœ… D) Output self-check

 Post validator escanea output

 Si violaciÃ³n:

regenerate 1x

si sigue â†’ redacciÃ³n automÃ¡tica + flag

ğŸ“Œ Meta: comportamiento explÃ­cito, sin debate.

âœ… TL;DR â€” Lo que le falta a tu generador actual para ser â€œperfectoâ€

SegÃºn lo que mostrÃ¡s en UI (oofos ejemplo), lo mÃ¡s importante que falta es:

Competitor guardrails por tamaÃ±o (employee/revenue)

Tiering y scoring de competidores (tier 1 vs tier 2)

Evidence pack (por quÃ© cada competidor estÃ¡ ahÃ­)

Context Quality score compuesto (explicable)

Versioning + context_hash

ValidaciÃ³n humana â€œsolo cuando confidence es bajaâ€

semantic_sensitivity + override TTL (para Negative Scope)